---
title: "GoldStandard"
author: "LauraFlorencia"
date: "1/17/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

# GOLD STANDARD: IS IT GOOD FOR TODAY'S ECONOMY? 
## Observation using Clustering and Dimension Reduction Method

This paper made to accomplish an assignment in Unsupervised Learning course in Data Science and Business Analytics major at the University of Warsaw.

The paper is related to Applied Microeconomy assignment that I am doing with my team. 

> Laura E. R. Florencia (430985) - 2021

---

In this paper, we are going to talk about "Gold Standard". Everybody have heard the notion, maybe in economics or to describe something to be high in value. This notion evolved throughout history and even though the actual gold standard is not used anymore as the basis of finance, we still see its signs everywhere. 

I am trying to cover Unsupervised Learning topic for Clustering and Dimension Reduction to get the analysis of how gold standard relates to our economic nowadays and whether it's good to be the current currency based from the data in 19 countries, in the US, Europe, Asia, Africa and Australia. 

Here is the data from gold.org for the gold prices per troy-ounce* from 1978-2020 in some countries (national Currency Unit per troy ounce)

> 1 troy ounce = 31.1034768 grams
> Troy ounce is a unit of measure used for weighing precious metals that dates back to the Middle Ages

```{r}
library(cluster)
library(factoextra)
library(flexclust)
library(fpc)
library(clustertend)
library(ClusterR)
library(readxl)
library(ggplot2)
library(dplyr)
library(plotly)
library(hrbrthemes)
library(animation) #for rgl in clusterSim
library(FactoMineR)

setwd("D:\\0. DSBA - Warsaw Uni\\8 Unsupervised Learning\\Paper")
getwd()

dataprice <- read.csv2("goldprice.csv")
datadaily <-read.csv2("dailyFluctuation.csv")
datacleardaily <- read.csv2("dailyFluctuationt.csv")
View(dataprice)
View(datadaily)
View(datacleardaily)
```
Let us see the the price fluctuation from 6 countries for 42 years. The prices were converted from local currency to USD.

```{r}
ggplot(data = dataprice, aes(x = dataprice$ï..year, y = dataprice$USD))+
    geom_line() + ggtitle("Gold Price in USA, 1978 - 2020")

ggplot(data = dataprice, aes(x = dataprice$ï..year, y = dataprice$EUR))+
  geom_line() + ggtitle("Gold Price in Europe, 1978 - 2020")

ggplot(data = dataprice, aes(x = dataprice$ï..year, y = dataprice$JPY))+
  geom_line() + ggtitle("Gold Price in Japan, 1978 - 2020")

ggplot(data = dataprice, aes(x = dataprice$ï..year, y = dataprice$GBP))+
  geom_line() + ggtitle("Gold Price in England, 1978 - 2020")

ggplot(data = dataprice, aes(x = dataprice$ï..year, y = dataprice$CAD))+
  geom_line() + ggtitle("Gold Price in Canada, 1978 - 2020")

ggplot(data = dataprice, aes(x = dataprice$ï..year, y = dataprice$CHF))+
  geom_line() + ggtitle("Gold Price in Swiss, 1978 - 2020")

```
#After showing all prices since 1978-2020, then go deeper into clustering 

The Elbow Curve method is helpful because it shows how increasing the number of the clusters contribute separating the clusters in a meaningful way, not in a marginal way. We also have another way to perform the method, which is Nbclust.

Nbclust proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.

```{r}
set.seed(42)
# function to compute total within-cluster sum of squares
fviz_nbclust(dataprice, kmeans, method = "wss", k.max = 25) + theme_minimal() + 
  ggtitle("The Elbow Method")
```

# Example of NbClust
However, in this paper we are not performed the NbClust

> set.seed(42)
> res.nbclust <- NbClust(dataprice, distance = "euclidean",
>                       min.nc = 2, max.nc = 25, 
>                       method = "complete", index ="all")
> factoextra::fviz_nbclust(res.nbclust) + theme_minimal() + ggtitle("NbClust's optimal number of clusters")

In this paper, we will performed 2 type of Unsupervised Learning, which is Clustering and Dimension Reduction. 
Clustering of dataset – by year (in rows after transposition). All data started from 1978 - 2020 (42 years). We will do 3x testing on clustering, start from k=3, just to test the theory. And we are also will find out the silhouette. 

```{r}
#k = 3

km3<-eclust(dataprice, "kmeans", hc_metric="euclidean", k=3)
fviz_cluster(km3, main="Yearly Price of Gold - 3 Clusters")
```
```{r}
km3$cluster
```
Check the silhouette value.
```{r}
km3$silinfo
```
If we see the negative silhouette, then it’s a problem somewhere, and while clustering is not perfect and we should have another clustering with different distance or we should find the different solution. In k=3, the negative silhouette is in cluster 3 from year 8, or at 1985.

```{r}
#k = 4

km4<-eclust(dataprice, "kmeans", hc_metric="euclidean", k=4)
fviz_cluster(km4, main="Yearly Price of Gold - 4 Clusters")
```
Cluster in k=4
```{r}
km4$cluster
```
Silhouette data
```{r}
km4$silinfo
```
In k=4, we still get the negative silhouette and the negative observation is double up than k=3. We will test on one more cluster and see the result.

```{r}
km5<-eclust(dataprice, "kmeans", hc_metric="euclidean", k=5)
fviz_cluster(km5, main="Yearly Price of Gold - 5 Clusters")
```
I am not prefer to choose k=5 because in year 31-32, the weight is not proportional rather than in k=3.
```{r}
km5$cluster
```
Last check for the k-value.
```{r}
km5$silinfo
```
The negative silhouette number is the same with k=3. But I choose k=3 to our main observation.

The price in k=3 devided perfectly and no overlap observation despite 1 negative silhouette. And in the observation, we can see that:
> Cluster 1: 33 - 42, from year 2008 - 2020 
> Cluster 2: 1, 2, 9-28, from year 1978 - 1979 and 1986 - 2003
> Cluster 3: 3 - 8, 29 - 32, from year 1980 - 1985 and 2004 - 2007

In the part Dimension Reduction using PCA and MFA, we will see what happened in those years, either recession or not and how we can conclude this condition into a suggestion for the Gold Standard. 

```{r}
#6 countries to become our sample from total 19 countries
dim(dataprice)
head(dataprice)
```

All basis observation countries. Some of them start late to adopt or produce gold in their country. These 19 countries are the largest gold reserves in the whole world. But here, I don't rank it from the biggest one (Australia, 10000 tonnes), but instead reffering to USA's conditions in economic market. 
```{r}
head(datacleardaily)
```
In some countries like China (CNY) and Canada (CAD), they start late and we start to get the data in 1985 from China. Starting in 1970, the economy entered into a period of stagnation, and after the death of Mao Zedong, the Communist Party leadership turned to market-oriented reforms to salvage the failing economy, and just from late 1980s, they start the transition and open up of the country to foreign investment and they're just start to produce gold at late 1980. 

![China Gold Production](D:/0. DSBA - Warsaw Uni/8 Unsupervised Learning/Paper/ChinaGold.png)

#CLARA from uncleared data
In this part, we are going to go more deeper into CLARA because we have a large dataset. 
```{r}
dim(datacleardaily)
```

We performing CLARA from our real dataset from 19 countries and the price is still using their local currency. CLARA clustering for this observation maybe not a good idea since the data overlapping, but let us perform this in the clear dataset.
```{r}
#cl2<-eclust(datadaily, "clara", k=5) # factoextra
summary(cl2)
fviz_cluster(cl2)
fviz_silhouette(cl2)
```
##Depending on observations drawn, silhouette may be slightly different
```{r}
fviz_cluster(cl2, geom="point", ellipse.type="norm") # factoextra::
fviz_cluster(cl2, palette=c("#00AFBB", "#FC4E07", "#E7B800"), ellipse.type="t", geom="point", pointsize=1, ggtheme=theme_classic())
fviz_silhouette(cl2)
```

#CLARA from Daily Data Price after exchange the local currency into USD
```{r}
cl2<-eclust(datacleardaily, "clara", k=3) # factoextra
summary(cl2)
fviz_cluster(cl2)
fviz_silhouette(cl2)
```
We can see from time to time the price scattered, the trend is not up or down or give a good traffic. So many outliers and uncertainty regarding the price.

#Dimension Reduction - PCA
##Data Normalization
First thing to do is normalize the data, we dont have to always do this as long our data is normalized.
install.packages("D:/Installer/clusterSim_0.49-2/clusterSim", repos = NULL, type = "source")
datacleardaily.n<-data.Normalization(datacleardaily, type="n1", normalization="column")#clusterSim::

##Find the eigenvalues on the basis of covariance
```{r}
datacleardaily.cov<-cov(datacleardaily.n)
datacleardaily.eigen<-eigen(datacleardaily.cov)
datacleardaily.eigen$values
head(datacleardaily.eigen$vectors)

#Perform the PCA function
cleardata<-datacleardaily

cleardata.pca1<-prcomp(10951, center=TRUE, scale.=FALSE) # stats::
cleardata.pca1
cleardata.pca1$rotation #only “rotation” part, the matrix of variable loadings

cleardata.pca2<-princomp(1) # stats::princomp()
loadings(cleardata.pca2)

plot(datacleardaily.pca2)
library(factoextra)
fviz_pca_var(datacleardaily.pca1, col.var="steelblue")# 

#Visualization of PCA
install.packages("D:/Installer/ggfortify_0.4.11/ggfortify", repos = NULL, type = "source")
library(ggfortify)
autoplot(datacleardaily.pca1)

autoplot(datacleardaily.pca1, loadings=TRUE, loadings.colour='blue', loadings.label=TRUE, loadings.label.size=3)
```
#Multiple Factor Analysis
Multiple factor analysis (MFA) (J. Pagès 2002) is a multivariate data analysis method for summarizing and visualizing a complex data table in which individuals are described by several sets of variables (quantitative and/ or qualitative) structured into groups. It takes into account the contribution of all active groups of variables to define the distance between individuals. The number of variables in each group may differ and the nature of the variables (qualitative or quantitative) can vary from one group to the other but the variables should be of the same nature in a given group (Abdi and Williams 2010).

##Why MFA? 
I chose MFA because the data mixes continuous and categorical variables. As far as groups go, it seemed like a good idea to combine all of the presence for the pricing both in recession and in the normal year.
```{r}
data(datacleardaily)
colnames(datacleardaily)
```
##Economic condition in the USA between year 1978 - 2020:
- group1: 1978 - 1982 > recession > 1046 rows
- group2: 1983 - 1989 > normal > 2872 - 1046 = 1826 rows
- group3: 1990 - 1991 > recession > 3394 - 2872 = 522 rows
- group4: 1992 - 2000 > normal > 5742 - 3394 = 2348 rows
- group5: 2001 > recession > 6003 - 5742 = 261 rows
- group6: 2002 - 2007 > normal > 7568 - 6003 = 1565 rows
- group7: 2008 - 2009 > recession > 8091 - 7568 = 523 rows
- group8: 2010 - 2019 > normal > 10699 - 8091 = 2608 rows
- group9: 2020 > recession > 10952 - 10699 = 253 rows

In MFA, we should determine the groups and make it into the nearest possibility and distance. Step by step to determine the groups:
1. First group: a group of categorical variables specifying the countries in the USA in first 2 columns in the data table. In FactoMineR terminology, the arguments group = 2 is used to define the first 2 columns as a group.
2. Second group: a group of countries in Europe for the next 4 columns, so the arguments group = 4.
3. Third group: a group contain of 3 countries in South East Asia
4. Forth group: 4 countries in Middle East (group = 4)
5. Fifth group: another ASEAN country (group = 4)
6. Sixth group: contain Africa and Australia as the biggest gold supplier (group = 2).
```{r}
library(FactoMineR)
res.mfa = MFA(datacleardaily, 
               group = c(2, 4, 3, 4, 4, 2), 
               type = c("n", "s", "s", "n", "n", "n"),
               name.group = c("US","Europe","East Asia", "Middle-East", "South East Asia","Aus & Afr"),
               num.group.sup = c(1, 6),
               graph = TRUE)

print(res.mfa)
```
And Eigenvalues is important matriks that we should know.
```{r}
library("factoextra")
eig.val <- get_eigenvalue(res.mfa)
head(eig.val)

fviz_screeplot(res.mfa)

fviz_mfa_var(res.mfa, "group")
```

# Conclusions
The conclusions of this experiments cannot be straightforward, because in economy, there are many things we should consider. But, in this observation, I observed that we can not rely on gold as an exchange rate because when the economy is struggele in the recession, fiat money is better in term of exchange rate . 

Technically these small average gold betas are driven by low gold-currency return correlations and by the fact that the currency return standard deviations are about one-half the size of the gold return standard deviation. Third, if gold was a good currency hedge the statistical fingerprint of this belief should be supported by high regression. However, for this universe of currencies, there seems to be little connection between currency returns and gold returns. Additionally, from a broad perspective the “gold up-currency down” idea sometimes misfires.
From 1978 to the present the U.S. dollar price of gold rose and the U.S. dollar depreciated against the Japanese yen. However, the Japanese yen price of gold rose and the Japanese yen appreciated against the U.S. dollar. (The Golden Dillema, 2013)

And about the method, clustering, outlier detection and dimension reduction are important in data analysis tasks. From forming this this paper, I knew that the difference between K-Means Euclidian and CLARA are quite big. But, if the most efficient algorithm and implementation was already chosen, the results from this paper may suggest that testing the performance of different distance metrics may be beneficial. Especially if the algorithm is expected to run more than once in PAM or MFA.  

# References:
- https://www.investopedia.com/terms/t/troyounce.asp#:~:text=A%20troy%20ounce%20is%20a,to%20the%20U.K.%20Royal%20Mint., access on 19 Dec 2020
- https://www.researchgate.net/publication/331663878_Clustering_and_dimension_reduction_for_mixed_variables
- https://stats.stackexchange.com/questions/288668/clustering-as-dimensionality-reduction
- https://www.rug.nl/research/portal/publications/clustering-and-dimension-reduction-for-mixed-variables(83435d87-2609-46f5-96fb-ec016515490a).html
- https://rpubs.com/mkhan/STDS_36103_Vignette
- https://www.youtube.com/watch?v=N5gYo43oLE8
- https://en.wikipedia.org/wiki/Cluster_analysis
- https://www.nsenergybusiness.com/news/largest-gold-reserves/
- https://en.wikipedia.org/wiki/Chinese_economic_reform
... many more
